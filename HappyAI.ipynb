{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwokW0305OM+mhLVz1Itci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jabbermcn/Codewars/blob/master/HappyAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdS0cxdOVFZ3"
      },
      "outputs": [],
      "source": [
        "Mikalai Mikhailouski (jabbermnc@gmail.com / @jabber_mcn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "zqV5lsz4VWPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "\n",
        "pygame.mixer.init()\n",
        "pygame.mixer.music.load('Pirates of the Caribbean - Main Theme [MIDIfind.com].mid')\n",
        "\n",
        "pygame.mixer.music.play()\n",
        "\n",
        "while pygame.mixer.music.get_busy():\n",
        "    continue\n"
      ],
      "metadata": {
        "id": "wGsA0i51Vae4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Task 3"
      ],
      "metadata": {
        "id": "LFHMfn7QVceE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "# Download the GPT-2 model (can be changed to any other GPT-2 model).\n",
        "gpt2.download_gpt2(model_name='355M')\n",
        "\n",
        "# Load the GPT-2 model.\n",
        "model_name = '355M'\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "\n",
        "# Generate text.\n",
        "text = gpt2.generate(sess, length=100, temperature=0.7, prefix='Product description: ')\n",
        "\n",
        "# Print the generated text.\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "-DQ4CeGeVdc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Task 4"
      ],
      "metadata": {
        "id": "2fPwMoBFVfQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "from cycleGAN import Generator\n",
        "\n",
        "# Load pre-trained generator model\n",
        "generator = Generator()\n",
        "generator.load_state_dict(torch.load('generator.pth'))\n",
        "generator.eval()\n",
        "\n",
        "# Define input image path and output image path\n",
        "input_path = 'IMG_7945.jpg'\n",
        "output_path = 'nighttime_image.jpg'\n",
        "\n",
        "# Load input image and apply transformation\n",
        "input_image = Image.open(input_path).convert('RGB')\n",
        "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "input_tensor = transform(input_image).unsqueeze(0)\n",
        "\n",
        "# Generate output image\n",
        "with torch.no_grad():\n",
        "    output_tensor = generator(input_tensor)\n",
        "output_image = transforms.ToPILImage()(output_tensor.squeeze(0))\n",
        "\n",
        "# Save output image\n",
        "output_image.save(output_path)\n"
      ],
      "metadata": {
        "id": "T7BmdxwTVfuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Task 5"
      ],
      "metadata": {
        "id": "PuLRucfFVi83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "\n",
        "# Load pre-trained VGG19 model\n",
        "model = models.vgg19(pretrained=True).features\n",
        "model.eval()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Define image preprocessing function\n",
        "def preprocess(image):\n",
        "    # Resize image to 512x512\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(512),\n",
        "        transforms.CenterCrop(512),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Preprocess image\n",
        "    image = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Move image to GPU if available\n",
        "    return image.to(device)\n",
        "\n",
        "\n",
        "# Define feature extraction function\n",
        "def get_features(image, model):\n",
        "    # Define layers to extract content and style features from\n",
        "    content_layers = ['conv_4']\n",
        "    style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "    # Dictionary to store content and style features\n",
        "    features = {'content': {}, 'style': {}}\n",
        "\n",
        "    # Loop through layers in model\n",
        "    for name, layer in model._modules.items():\n",
        "        image = layer(image)\n",
        "\n",
        "        # If layer is in content_layers, store its features\n",
        "        if name in content_layers:\n",
        "            features['content'][name] = image\n",
        "\n",
        "        # If layer is in style_layers, store its features\n",
        "        if name in style_layers:\n",
        "            features['style'][name] = gram_matrix(image)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Define Gram matrix function\n",
        "def gram_matrix(tensor):\n",
        "    b, c, h, w = tensor.size()\n",
        "    features = tensor.view(b * c, h * w)\n",
        "    gram = torch.mm(features, features.t())\n",
        "    return gram.div(b * c * h * w)\n",
        "\n",
        "\n",
        "# Define style transfer function\n",
        "def style_transfer(content_image, style_image, num_iterations=1000, content_weight=1, style_weight=100000):\n",
        "    # Preprocess content and style images\n",
        "    content = preprocess(content_image)\n",
        "    style = preprocess(style_image)\n",
        "\n",
        "    # Extract content and style features\n",
        "    content_features = get_features(content, model)\n",
        "    style_features = get_features(style, model)\n"
      ],
      "metadata": {
        "id": "c3EB8kcVVjyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Task 6"
      ],
      "metadata": {
        "id": "CF25EkXxVmhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from manim import *\n",
        "import random\n",
        "\n",
        "\n",
        "class BouncingBall(Scene):\n",
        "    def construct(self):\n",
        "        # Set up the ball\n",
        "        ball = Circle(radius=0.5, fill_opacity=1, fill_color=BLUE).shift(UP * 3)\n",
        "\n",
        "        # Set up the floor\n",
        "        floor = Line(start=LEFT * 5, end=RIGHT * 5).shift(DOWN * 2.5)\n",
        "\n",
        "        # Set up the initial velocity and acceleration of the ball\n",
        "        velocity = 0\n",
        "        acceleration = -9.8\n",
        "\n",
        "        # Set up the time step\n",
        "        dt = 0.1\n",
        "\n",
        "        # Animate the ball\n",
        "        while ball.get_bottom()[1] >= floor.get_top()[1]:\n",
        "            # Update the position of the ball\n",
        "            ball.shift(velocity * dt * UP + 0.5 * acceleration * dt ** 2 * DOWN)\n",
        "            velocity += acceleration * dt\n",
        "\n",
        "            # Randomly change the color of the ball\n",
        "            ball.set_color(random.choice([RED, BLUE, GREEN, YELLOW]))\n",
        "\n",
        "            self.add(ball, floor)\n",
        "            self.wait(dt)\n",
        "\n",
        "        self.play(FadeOut(ball))\n"
      ],
      "metadata": {
        "id": "uF5uz5GbVnQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Task 7"
      ],
      "metadata": {
        "id": "NXxcmT1SVp44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "# Load pre-trained InceptionV3 model\n",
        "model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "model.trainable = False\n",
        "\n",
        "\n",
        "# Define image preprocessing function\n",
        "def preprocess(image):\n",
        "    # Resize image to 299x299\n",
        "    image = tf.image.resize(image, (299, 299))\n",
        "\n",
        "    # Normalize image with ImageNet mean and standard deviation values\n",
        "    image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# Define layer loss function\n",
        "def calculate_loss(layer_outputs):\n",
        "    losses = []\n",
        "    for output in layer_outputs:\n",
        "        loss = tf.math.reduce_mean(tf.square(output))\n",
        "        losses.append(loss)\n",
        "    return tf.reduce_sum(losses)\n",
        "\n",
        "\n",
        "# Define deep dream function\n",
        "def deep_dream(image, octaves=6, octave_scale=1.3, iterations=10, step_size=0.01):\n",
        "    # Convert image to a TensorFlow tensor\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    image = tf.keras.preprocessing.image.smart_resize(image, (224, 224))\n",
        "    image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
        "    image = tf.convert_to_tensor(image)\n",
        "\n",
        "    # Define list of layers to use for optimization\n",
        "    names = ['mixed3', 'mixed5', 'mixed7']\n",
        "    layers = [model.get_layer(name).output for name in names]\n",
        "\n",
        "    # Build the feature extraction model\n",
        "    dream_model = tf.keras.Model(inputs=model.input, outputs=layers)\n",
        "\n",
        "    # Define the octave scales\n",
        "    octave_sizes = [tf.shape(image)[:2]]\n",
        "    for i in range(1, octaves):\n",
        "        size = tf.cast(tf.floor(octave_sizes[-1] / octave_scale), tf.int32)\n",
        "        octave_sizes.append(size)\n",
        "\n",
        "    # Generate the deep dream image\n",
        "    for octave_size in octave_sizes[::-1]:\n",
        "        # Resize the image to the current octave size\n",
        "        image = tf.image.resize(image, octave_size)\n",
        "\n",
        "        for i in range(iterations):\n",
        "            # Optimize the image to maximize layer outputs\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(image)\n",
        "                outputs = dream_model(preprocess(image))\n",
        "                loss = calculate_loss(outputs)\n",
        "\n",
        "            gradients = tape.gradient(loss, image)\n",
        "            gradients /= tf.math.reduce_std(gradients) + 1e-8\n",
        "            image = image + gradients * step_size\n",
        "            image = tf.clip_by\n",
        "\n",
        "\n",
        "preprocess('IMG_7945.jpg')\n"
      ],
      "metadata": {
        "id": "ltJ-v3iBVqqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}